{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /datasets/home/64/364/rhadden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from decoder import *\n",
    "from encoder import *\n",
    "from other_data_loader import *\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import torchvision.transforms as tf\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_test(val_loader, encoder, decoder, criterion, maxSeqLen,\n",
    "             vocab, batch_size, use_gpu = True, calculate_bleu = True):\n",
    "\n",
    "    \n",
    "    #Evaluation Mode\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    \n",
    "    references = list()\n",
    "    hypotheses = list() \n",
    "   \n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        count    = 0\n",
    "        loss_avg = 0\n",
    "        bleu1_avg = 0\n",
    "        bleu4_avg = 0\n",
    "                \n",
    "        for i, (inputs, caps, allcaps) in enumerate(val_loader):\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Move to device, if available\n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)\n",
    "                caps = caps.to(device)\n",
    "\n",
    "                        \n",
    "            enc_out = encoder(inputs)\n",
    "            actual_lengths = allcaps\n",
    "            \n",
    "            \n",
    "            \n",
    "            temperature = 1\n",
    "            test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature)\n",
    "\n",
    "            test_pred_sample = test_pred[0].cpu().numpy()          \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            #Build a list of the predicted sentences\n",
    "            # Convert word_ids to words\n",
    "            sampled_caption = []\n",
    "\n",
    "            for word_id in test_pred_sample:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                sampled_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            sentence = ' '.join(sampled_caption)\n",
    "            hypotheses.append(sampled_caption)                \n",
    "            #if i % 750 ==0:\n",
    "                #print ('generated sentence: ',sentence)            \n",
    "                #print(type(sampled_caption))        \n",
    "                #print(sampled_caption)        \n",
    "                #print('len(generated_sentence): ',len(sampled_caption))\n",
    "\n",
    "                \n",
    "            #targets = pack_padded_sequence(labels, lengths, batch_first=True)[0]\n",
    "            \n",
    "            \n",
    "            decoder.resetHidden(inputs.shape[0])\n",
    "            outputs = decoder(caps, enc_out, actual_lengths)\n",
    "#             if i % 1000 == 0:\n",
    "#                 print('VAL: outputs shape: ', outputs.size())\n",
    "#            new_outputs = torch.zeros(inputs.shape[0], maxSeqLen, vocab.idx)\n",
    "#            new_outputs[:inputs.shape[0],:maxSeqLen, 0] = torch.ones((inputs.shape[0], maxSeqLen))\n",
    "#             for dim in range(maxSeqLen):\n",
    "#                 for b in range(inputs.shape[0]):\n",
    "#                     new_outputs[b, dim, 0] = 1.0\n",
    "#            new_outputs[:, :(outputs.shape[1]), :] = outputs\n",
    "#            new_outputs = new_outputs.permute(0, 2, 1).to(device)\n",
    "            \n",
    "            #del inputs\n",
    "\n",
    "            \n",
    "            loss = criterion(outputs, Variable(caps.long()))\n",
    "            loss_avg += loss\n",
    "            count+=1\n",
    "            \n",
    "            #del outputs            \n",
    "            \n",
    "            #print('VAL: loss: ', loss)\n",
    "\n",
    "\n",
    "            caps_array = caps.cpu().numpy()  \n",
    "            # Convert word_ids to words\n",
    "            reference_caption = []\n",
    "            sampled_caption = []\n",
    "            \n",
    "            for word_id in caps_array[0]:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                reference_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            ref_sentence = ' '.join(reference_caption)\n",
    "            #if i % 500 == 0:\n",
    "                #print('ref_sentence: ', ref_sentence)\n",
    "                #print('len(ref_sentence): ',len(reference_caption))\n",
    "            references.append(reference_caption)   \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            #print('len(references)', len(references))\n",
    "            #print('len(hypotheses)', len(hypotheses))\n",
    "            #print('references: ', references)\n",
    "            #print('hypotheses: ', hypotheses)\n",
    "        \n",
    "\n",
    "            # Calculate BLEU-4 scores\n",
    "            if calculate_bleu:\n",
    "                bleu4 = corpus_bleu(references, hypotheses)                \n",
    "                bleu1 = corpus_bleu(references, hypotheses,weights=(1.0, 0, 0, 0))\n",
    "                #print('bleu4: ', bleu4)        \n",
    "                #print('bleu1: ', bleu1)  \n",
    "                bleu4_avg+=bleu4\n",
    "                bleu1_avg+=bleu1\n",
    "            \n",
    "            \n",
    "            \n",
    "            del caps\n",
    "            del outputs            \n",
    "            \n",
    "            \n",
    "            #if i % 10 == 0:\n",
    "            #    break\n",
    "                \n",
    "        loss_avg  = loss_avg/count\n",
    "        print('VAL: loss_avg: ', loss_avg)\n",
    "\n",
    "        if calculate_bleu:\n",
    "            \n",
    "            bleu4_avg = bleu4_avg/count\n",
    "            bleu1_avg = bleu1_avg/count \n",
    "            \n",
    "            print('VAL: bleu4_avg: ', bleu4_avg)\n",
    "            print('VAL: bleu1_avg: ', bleu1_avg)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    return loss_avg, bleu1_avg, bleu4_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        train_loader,val_loader, test_loader,\n",
    "                        name, batch_size, maxSeqLen, vocab,save_generated_imgs= False):\n",
    "    \n",
    "    #Create non-existing logfiles\n",
    "    logname = './logs/' + name + '.log'\n",
    "    i = 0\n",
    "    if os.path.exists(logname) == True:\n",
    "        \n",
    "        logname = './logs/' + name + str(i) + '.log'\n",
    "        while os.path.exists(logname):\n",
    "            i+=1\n",
    "            logname = './logs/' + name + str(i) + '.log'\n",
    "\n",
    "    print('Loading results to logfile: ' + logname)\n",
    "    with open(logname, \"w\") as file:\n",
    "        file.write(\"Log file DATA: Validation Loss and Accuracy\\n\") \n",
    "    \n",
    "    logname_summary = './logs/' + name + '_summary' + str(i) + '.log'    \n",
    "    print('Loading Summary to : ' + logname_summary) \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir('./generated_imgs')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    generated_imgs_filename = './generated_imgs/generated_imgs' + name + '_summary' + str(i) + '.log'\n",
    "    \n",
    "    \n",
    "    parameters = list(encoder.fc.parameters())\n",
    "    parameters.extend(list(decoder.parameters()))\n",
    "    optimizer = optim.Adam(parameters, lr=5e-5)\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "#         encoder = torch.nn.DataParallel(encoder)\n",
    "#         decoder = torch.nn.DataParallel(decoder)\n",
    "        \n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        \n",
    "        \n",
    "    \n",
    "    val_loss_set = []\n",
    "    val_bleu1_set = []\n",
    "    val_bleu4_set = []\n",
    "    \n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    # Early Stop criteria\n",
    "    minLoss = 1e6\n",
    "    minLossIdx = 0\n",
    "    earliestStopEpoch = 10\n",
    "    earlyStopDelta = 5\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "        print(\"Type\")    \n",
    "\n",
    "        print(type(train_loader))\n",
    "        for iter, (inputs, labels, lengths) in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)# Move your inputs onto the gpu\n",
    "                labels = labels.to(device) # Move your labels onto the gpu\n",
    "            \n",
    "                \n",
    "            enc_out = encoder(inputs)\n",
    "            \n",
    "            temperature = 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            decoder.resetHidden(inputs.shape[0])\n",
    "            outputs = decoder(labels, enc_out, lengths) #calls forward\n",
    "            #targets = pack_padded_sequence(labels, lengths, batch_first=True)\n",
    "            #targets = pack_padded_sequence(labels, actual_lengths, batch_first=True, enforce_sorted=False)\n",
    "            #targets = torch.zeros()\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "            del labels\n",
    "            del outputs\n",
    "\n",
    "            loss.backward()\n",
    "#             loss = loss#.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 200 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss))\n",
    "                #break\n",
    "#                 test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature).cpu()\n",
    "#                 for b in range(inputs.shape[0]):\n",
    "#                     caption = (\" \").join([vocab.idx2word[x.item()] for x in test_pred[b]])\n",
    "#                     img = tf.ToPILImage()(inputs[b,:,:,:].cpu())\n",
    "#                     plt.imshow(img)\n",
    "                    \n",
    "#                     plt.show()\n",
    "#                     print(\"Caption: \" + caption)\n",
    "                \n",
    "        print(\"epoch{}, iter{}, loss: {}, epoch duration: {}\".format(epoch, iter, loss, time.time() - ts))\n",
    "        test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature).cpu()\n",
    "        \n",
    "        k = 0\n",
    "        for b in range(inputs.shape[0]):\n",
    "            caption = (\" \").join([vocab.idx2word[x.item()] for x in test_pred[b]])\n",
    "            img = tf.ToPILImage()(inputs[b,:,:,:].cpu())\n",
    "            plt.imshow(img)\n",
    "                    \n",
    "            plt.show()\n",
    "            print(\"Caption: \" + caption)\n",
    "            if save_generated_imgs:\n",
    "                file = \"./generated_imgs/\" + \"train_epoch\" + str(epoch) + \"im_\"+ str(k) \n",
    "                img.save(file + \".png\", \"PNG\")\n",
    "                k+=1\n",
    "                with open(generated_imgs_filename, \"a\") as file:\n",
    "                    file.write(\"writing! \" + \"train_epoch\" + str(epoch) + \"im_\"+ str(k) + \"\\n\")            \n",
    "                    file.write(\"Caption: \" + caption +\"\\n \\n\")\n",
    "        \n",
    "        # calculate val loss each epoch\n",
    "        val_loss, val_bleu1, val_bleu4  = validate_test(val_loader, encoder, decoder, criterion,maxSeqLen,\n",
    "                             vocab, batch_size, use_gpu, calculate_bleu = False)\n",
    "        val_loss_set.append(val_loss)\n",
    "        val_bleu1_set.append(val_bleu1)\n",
    "        val_bleu4_set.append(val_bleu4)\n",
    "\n",
    "        \n",
    "#         print(\"epoch {}, time {}, train loss {}, val loss {}, val acc {}, val iou {}\".format(epoch, time.time() - ts,\n",
    "#                                                                                                loss, val_loss,\n",
    "#                                                                                                val_acc,\n",
    "#                                                                                                val_iou))        \n",
    "        training_loss.append(loss)\n",
    "        \n",
    "        torch.save(encoder, 'weights_base/encoder_epoch{}'.format(epoch))\n",
    "        torch.save(decoder, 'weights_base/decoder_epoch{}'.format(epoch))\n",
    "        \n",
    "        with open(logname, \"a\") as file:\n",
    "            file.write(\"writing!\\n\")\n",
    "            file.write(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "            file.write(\"\\n training Loss:   \" + str(loss.item()))\n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set[-1]))\n",
    "            file.write(\"\\n Validation bleu1:  \" + str(val_bleu1_set[-1]))\n",
    "            file.write(\"\\n Validation bleu4:  \" + str(val_bleu4_set[-1]) + \"\\n \")                                            \n",
    "                                                                                                \n",
    "                                                                                                \n",
    "        \n",
    "        # Early stopping\n",
    "#         if val_loss < minLoss:\n",
    "#             # Store new best\n",
    "#             torch.save(model, name)\n",
    "#             minLoss = val_loss#.item()\n",
    "#             minLossIdx = epoch\n",
    "            \n",
    "        # If passed min threshold, and no new min has been reached for delta epochs\n",
    "#         elif epoch > earliestStopEpoch and (epoch - minLossIdx) > earlyStopDelta:\n",
    "#             print(\"Stopping early at {}\".format(minLossIdx))\n",
    "#             break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        with open(logname_summary, \"w\") as file:\n",
    "            file.write(\"Summary!\\n\")\n",
    "            #file.write(\"Stopped early at {}\".format(minLossIdx))\n",
    "            file.write(\"\\n training Loss:   \" + str(training_loss))        \n",
    "            file.write(\"\\n Validation Loss : \" + str(val_loss_set))\n",
    "            file.write(\"\\n Validation bleu1:  \" + str(val_bleu1_set))\n",
    "            file.write(\"\\n Validation bleu4:  \" + str(val_bleu4_set) + \"\\n \")\n",
    "            \n",
    "        \n",
    "    #return val_loss_set, val_acc_set, val_iou_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 7323 train ids\n",
      "found 866 test ids\n",
      "# ids: 58590\n",
      "# ids: 14640\n",
      "# ids: 8660\n",
      "Loading results to logfile: ./logs/LSTM2.log\n",
      "Loading Summary to : ./logs/LSTM_summary2.log\n",
      "Type\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter0, loss: 7.545274257659912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [00:24,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter200, loss: 1.0059680938720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "402it [00:49,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter400, loss: 0.5710200071334839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:14,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter600, loss: 0.5080430507659912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "802it [01:38,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter800, loss: 0.7432484030723572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [02:03,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter1000, loss: 0.4593638777732849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1202it [02:27,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter1200, loss: 0.5876981616020203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [02:52,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter1400, loss: 0.4628753066062927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1602it [03:16,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter1600, loss: 0.6250199675559998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1802it [03:41,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter1800, loss: 0.5417322516441345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [04:05,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter2000, loss: 0.4870154857635498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2202it [04:30,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter2200, loss: 0.48893558979034424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2402it [04:54,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter2400, loss: 0.6270290017127991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2601it [05:18,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter2600, loss: 0.5298337936401367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2802it [05:43,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter2800, loss: 0.42725706100463867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3002it [06:07,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter3000, loss: 0.4083181321620941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3202it [06:31,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter3200, loss: 0.4104045629501343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3402it [06:56,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter3400, loss: 0.40149155259132385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3602it [07:20,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter3600, loss: 0.43959739804267883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3802it [07:44,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter3800, loss: 0.356599897146225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [08:09,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter4000, loss: 0.599996030330658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4201it [08:33,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter4200, loss: 0.508206307888031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4333it [08:49,  8.02it/s]"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('trainvalIds.csv', 'r') as f:\n",
    "        trainIds = []\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                trainIds.append(line.strip(\"\\n\"))\n",
    "\n",
    "        \n",
    "    with open('testIds.csv', 'r') as f:\n",
    "        testIds = []\n",
    "        for line in f:\n",
    "            if len(line) > 1:\n",
    "                testIds.append(line.strip(\"\\n\"))\n",
    "    \n",
    "    print(\"found {} train ids\".format(len(trainIds)))\n",
    "    print(\"found {} test ids\".format(len(testIds)))\n",
    "    \n",
    "    # Will shuffle the trainIds incase of ordering in csv\n",
    "    random.shuffle(trainIds)\n",
    "    splitIdx = int(len(trainIds)/5)\n",
    "    \n",
    "    # Selecting 1/5 of training set as validation\n",
    "    valIds = trainIds[:splitIdx]\n",
    "    trainIds = trainIds[splitIdx:]\n",
    "    #print(trainIds)\n",
    "    \n",
    "    \n",
    "    trainValRoot = \"./data/realImages/\"\n",
    "    testRoot = \"./data/realImages/\"\n",
    "    \n",
    "    trainValCaps = \"./data/captions/trainvalCaps.csv\"\n",
    "    testCaps = \"./data/captions/testCaps.csv\"\n",
    "    \n",
    "    \n",
    "    with open('./data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    img_side_length = 256\n",
    "    transform = tf.Compose([\n",
    "        tf.Resize(img_side_length),\n",
    "        #tf.RandomCrop(img_side_length),\n",
    "        tf.CenterCrop(img_side_length),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    batch_size = 10\n",
    "    shuffle = True\n",
    "    num_workers = 5\n",
    "    \n",
    "    \n",
    "    trainDl = get_loader(trainValRoot, trainValCaps, trainIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=shuffle, num_workers=num_workers)\n",
    "    valDl = get_loader(trainValRoot, trainValCaps, valIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=shuffle, num_workers=num_workers)\n",
    "    testDl = get_loader(testRoot, testCaps, testIds, vocab, \n",
    "                        transform=transform, batch_size=batch_size, \n",
    "                        shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    encoded_feature_dim = 1024\n",
    "    maxSeqLen = 56\n",
    "    hidden_dim = 1500\n",
    "    depth = 1\n",
    "    \n",
    "    encoder = Encoder(encoded_feature_dim)\n",
    "    # Turn off all gradients in encoder\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Turn on gradient of final hidden layer for fine tuning\n",
    "    for param in encoder.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    decoder = Decoder(encoded_feature_dim, hidden_dim, depth, vocab.idx, batch_size)\n",
    "    \n",
    "#     criterion = nn.NLLLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        trainDl, valDl, testDl, \"LSTM\",\n",
    "                        batch_size, maxSeqLen, vocab,save_generated_imgs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
